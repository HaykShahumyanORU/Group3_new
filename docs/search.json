[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Group3",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nIt holds all the data for group 3",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Hayk.html",
    "href": "Hayk.html",
    "title": "1  Hayk",
    "section": "",
    "text": "1.1 Week 1\nExcel",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#week-1",
    "href": "Hayk.html#week-1",
    "title": "1  Hayk",
    "section": "",
    "text": "1.1.1 Wednesday\nI spent about 30 mins to an hour working through the VIZ_basic-excel-web excel practice page and did everything until the homework tab which requires me to do a different data set. So all steps 1 through 6 including all the basic functions, how to filter out tools that were not necessary, how to adjust inputted graphs and visualizations, and how to add in all the different visualizations were completed.\nsince this was my first time learning how to use quarto after finishing all the excel practices from Monday, I have not explained each individual tool I practiced since I have completed them all already.\n\n\n\n1.1.2 Friday\nToday I have chosen my data set. My data set has a large collection of data, which holds data of modeled medium and heavy duty vehicle with info on their stock, sales, energy consumption, greenhouse gas emissions, and total cost of driving. I have chosen to do vehicle sales as my dataset for today. Chatgpt recommends these areas for the inserts 1. Total Vehicle Sales by Year and Vehicle Class\nGraph Type: Stacked Bar Chart\nPurpose: This will show how the sales of different vehicle classes contribute to the total sales each year, and how the sales have changed over the years.\nX-axis: Year (2023, 2024, 2025, 2026)\nY-axis: Vehicle Sales (in thousands)\nStacked Bars: Light-Medium (Class 3), Medium (Classes 4-6), Heavy (Classes 7-8)\n\nTrend of Vehicle Sales by Vehicle Class Over Time\nGraph Type: Line Chart Purpose: To visualize the trend of sales for each vehicle class over the years. X-axis: Year (2023, 2024, 2025, 2026) Y-axis: Vehicle Sales (in thousands) Lines: Separate lines for Light-Medium (Class 3), Medium (Classes 4-6), and Heavy (Classes 7-8)\nYearly Distribution of Vehicle Sales Across Classes\nGraph Type: Pie Charts (for each year) Purpose: To show the percentage share of each vehicle class in total sales for each year. Slices: Light-Medium (Class 3), Medium (Classes 4-6), Heavy (Classes 7-8)\nComparison of Vehicle Sales Growth Between Classes\nGraph Type: Grouped Bar Chart Purpose: To compare how much each vehicle class has grown or declined in sales from year to year. X-axis: Year (2023, 2024, 2025, 2026) Y-axis: Vehicle Sales (in thousands) Bars: Separate bars for Light-Medium (Class 3), Medium (Classes 4-6), and Heavy (Classes 7-8), grouped by year.\nSales Proportion Change Over Years for Each Vehicle Class\nGraph Type: 100% Stacked Area Chart Purpose: To show how the proportion of sales for each vehicle class has changed over the years. X-axis: Year (2023, 2024, 2025, 2026) Y-axis: Percentage of Total Sales (%) Areas: Light-Medium (Class 3), Medium (Classes 4-6), Heavy (Classes 7-8)\n\nRecommended Chart: Total Vehicle Sales by Year and Vehicle Class (Stacked Bar Chart)\nThis chart provides a clear visual comparison of vehicle sales over the years and how each class contributes to the total sales.\n\nthis graph shows us that the light-medium vehicles are the ones that were sold the most from the large amount of data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#week-2",
    "href": "Hayk.html#week-2",
    "title": "1  Hayk",
    "section": "1.2 Week 2",
    "text": "1.2 Week 2\n\n1.2.1 Wednesday\n\n1.2.1.1 Histogram\nI am using the airquality dataset, which is …\nI use excel to clean the data, remove all the rows which have NA values, and I did an exploration analysis\n\n\n1.2.1.2 Ozone Histogram\n\n\n\n1.2.1.3 Ozone vs Temp\n #### which day out of all the months had the most solar R\n\n\n\n\n1.2.2 friday\nToday I am learning about pivot charts which are extended from pivot tables. To get to my insert chart I selected all of my pivot table and near the insert tab there was an option for a pivot chart.\n\nThis graph shows how for each day for each month, the ratio of each value. For example, for each month you can start to see the temperature changes between each day within each month. You can see which days have more average Ozone, and which days have an increase of solar Radiation. We can see how when there is an increase in temperature, there is less solar radiation which I find interesting. there was a day on the 8th month where there seemed to be no solar radiation and only an increased average temp and increased ozone.\n\nThis pivot table and pivot chart for my data is representing how much of the market is taken up by which type of vehicles. We can see visually that generally, there is a trend that the shorter the distance for the commercial vehicles, the more of those types of vehicles are bought. My data for this week is the vehicle stock. The X line represents how much vehicles are sold, while the Y line lists the amount of miles expected for that commercial vehicle type.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#week-3",
    "href": "Hayk.html#week-3",
    "title": "1  Hayk",
    "section": "1.3 Week 3",
    "text": "1.3 Week 3\n\n1.3.1 Wednesday\nA data frame with 20 observations on 3 variables.\n[, 1] extra numeric increase in hours of sleep [, 2] group factor drug given [, 3] ID factor patient ID\nUsing the sleep dataset to create my first dashboard\nhttps://public.tableau.com/app/profile/hayk.shahumyan/viz/Wednesdayweek3/Dashboard1\n\n\n1.3.2 Friday\nI plan to work on fuel costs of vehicles of projected till 2040\nFormat\nThere are 3 variables we focus on: cost of electricity cost of diesel cost of hydrogen from now till 2050\nDetails\nWe can see trends of downward prices of electricity and hydrogen, while diesel is mostly stable in price\nhttps://public.tableau.com/app/profile/hayk.shahumyan/viz/Week3Friday-Saturday/Dashboard1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#week-4",
    "href": "Hayk.html#week-4",
    "title": "1  Hayk",
    "section": "1.4 Week 4",
    "text": "1.4 Week 4\n\n1.4.1 Wednesday and Friday",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#this-is-a-markdown-title",
    "href": "Hayk.html#this-is-a-markdown-title",
    "title": "1  Hayk",
    "section": "1.5 This is a markdown title",
    "text": "1.5 This is a markdown title\nin markdown we can create lists:\n\nItem 1\nitem 2\nitem 3\n\nalso we can create enumerated list\n\nHola\nHi\nNamaste\n\nwe can do bold, also italic\n# Here we are importing numpy with a nickname np\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n1\n[1 2 3 4 5]\n#List are native to python\nmy_list = [1, 2, 3, 4, 5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n# We will be using a lot of dataframes, so we need pandas library\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#loading-csv-files",
    "href": "Hayk.html#loading-csv-files",
    "title": "1  Hayk",
    "section": "1.6 4. Loading csv Files",
    "text": "1.6 4. Loading csv Files\nTo load .csv files into a DataFrame, we use the pandas function read_csv:\ndf = pd.read_csv('airquality_datasets.csv')\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#visualizing-the-dataset",
    "href": "Hayk.html#visualizing-the-dataset",
    "title": "1  Hayk",
    "section": "1.7 5. visualizing the dataset",
    "text": "1.7 5. visualizing the dataset\nLet’s dive into visualizations using matplotlib. We’ll start with simple histograms and boxplots, then move on to correlation plots. Histograms\nHistograms help us understand the distribution of the variables. We’ll create histograms for Ozone and Temp.\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#week-5",
    "href": "Hayk.html#week-5",
    "title": "1  Hayk",
    "section": "1.8 Week 5",
    "text": "1.8 Week 5",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Hayk.html#week-after-fall-break",
    "href": "Hayk.html#week-after-fall-break",
    "title": "1  Hayk",
    "section": "1.9 Week after fall break",
    "text": "1.9 Week after fall break\n(For this weeks assignment, everyone needed to find a dataset from our datasource and give a summary about it.)\nhttps://www.bls.gov/wsp/overview.htm\nThe data provides a historical record of annual work stoppages in the U.S. involving 1,000 or more workers, starting from 1947. It includes the following key information:\nThe year of the work stoppage.\nThe number of work stoppages that began in a given year and those still in effect.\nThe number of workers involved (in thousands).\nThe number of days of idleness (in thousands) caused by these stoppages.\nThe percentage of total working time lost due to these stoppages.\nThis dataset is focused on capturing the frequency and economic impact of large-scale labor disruptions over time.\ndirect summary from the website:\nThe Bureau of Labor Statistics has two types of data about work stoppages: Work Stoppages program data and the Strike Report.\nThe Work Stoppages program provides monthly and annual data of major work stoppages involving 1,000 or more workers lasting one full shift or longer. The monthly and annual data show the establishment and union(s) involved in the work stoppage along with the location, the number of workers and the days of idleness. The monthly data list all work stoppages involving 1,000 or more workers that occurred during the full calendar month for each month of the year. The annualized data provide statistics, analysis and details of each work stoppage of 1,000 or more workers that occurred during the year. The work stoppages data are gathered from public news sources, such as newspapers and the Internet. The BLS does not distinguish between strikes and lock-outs in the data; both are included in the term “work stoppages”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hayk</span>"
    ]
  },
  {
    "objectID": "Pratiti.html",
    "href": "Pratiti.html",
    "title": "2  Pratiti",
    "section": "",
    "text": "2.1 Week 1\nExcel",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#week-1",
    "href": "Pratiti.html#week-1",
    "title": "2  Pratiti",
    "section": "",
    "text": "2.1.1 Wednesday\n\n2.1.1.1 Basic Functions\nI can now use sum, average, and more functions to select a whole row and make the process more efficient for large datasets.\n\n\n2.1.1.2 Basic Visualizations\nThrough using the insert tab, I learned how to choose to represent data with a scatterplot, line plot, bar chart, and more. To customize and format the histogram is easily done in Excel. Each axis can be labeled for the person viewing to understand.\n\n\n2.1.1.3 Scatter Plot\n\nFrom the Dataset, we see a trend that as the age of the tree increases, the circumference also increases. This is consistent for different types of trees and helps us see into history of how long a tree survived. Another method is also to count the rings once the tree is cut down. However, using the circumference gives a qualitative measurement of when the tree can still be alive and aged.\n\n\n\n2.1.2 Friday\nThe US Bureau of Labor and Statistics offers many data sets to do with the work force in America. This data is found from civilian unemployment rates over the past 10 years including people of different ethnicity.\n\nThe visualization shows there is a peak in unemployment rate in the year 2020-2021 when Covid-19 affected everyday lives of civilians. This graph represents the unemployment dramatic decrease during the pandemic, but also stabilization in 2022.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#week-2",
    "href": "Pratiti.html#week-2",
    "title": "2  Pratiti",
    "section": "2.2 Week 2",
    "text": "2.2 Week 2\n\n2.2.1 Wednesday\n\n2.2.1.1 Histogram\nI am using the air quality data set which is shows the ozone ppm distribution based on temperature, month and day. Th data was obtained from the New York State Department of Conservation and the National Weather Service. The temperature was taking as the maximum daily temperature at LaGuardia airport. The mean ozone is measured in parts per billion from 13:00 to 15:00 at Roosevelt Island.\nI used excel to clean the data through filtration by removing the NA values. I then did exploration analysis.\n\n\n2.2.1.2 Ozone Histogram\n\n\n\n2.2.1.3 Ozone vs. Temp Scatterplot\n\n\n\n2.2.1.4 My First Pivot Table\n\nThis table shows the difference in averages of ozone in May versus September. The trend shows that the average seems to be higher at the start of the month and decreases gradually. In September there is a general trend of higher ozone levels per day compared to May. This trend may reveal something new about the ozone levels and air quality.\n\n\n\n2.2.2 Friday\nU.S. Bureau of Labor Statistics\nThe Consumer Expenditure Surveys (CE) are national studies by the U.S. Bureau of Labor Statistics (BLS) that examine how people in the U.S. spend their money. The CE includes two types of surveys: the Interview Survey and the Diary Survey. The Interview Survey gathers information about significant or ongoing expenses that people can remember over a period of 3 months or more, including large purchases like homes and cars or regular payments like rent and utilities.\nCE data are valuable for both government and private organizations that focus on specific population groups, including the elderly, low-income families, urban residents, and individuals receiving Supplemental Nutrition Assistance Program (SNAP) benefits. Economic policymakers use this data to evaluate how policy changes affect living standards among various socioeconomic groups, while econometricians leverage it to build models for predicting economic outcomes.\nI plan to break up all the parts of the data to the main sections to compare how much consumers spend on each category in life.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#week-3",
    "href": "Pratiti.html#week-3",
    "title": "2  Pratiti",
    "section": "2.3 Week 3",
    "text": "2.3 Week 3\n\n2.3.1 Wednesday\n\n2.3.1.1 Orchard Sprays\nAn experiment was conducted to evaluate the effectiveness of various orchard spray components in repelling honeybees, using a Latin square design. In this experiment, dry comb cells were filled with specific amounts of lime sulfur emulsion mixed in sucrose solution. Seven concentrations of lime sulfur were tested, ranging from 1/100 to 1/1,562,500, with each concentration decreasing by factors of 1/5, along with a control solution containing no lime sulfur. A had the highest level of lime sulpher while H had no lime sulphur. To assess the bees’ responses, 100 bees were released into a chamber for two hours, and the decrease in solution volume in each cell was measured to determine the repellency of the different concentrations.\nDashboard on Tableau\n\n\n\n2.3.2 Friday\n\nPratiti will work on employment and wages in different states collectively -\n\nThe Quarterly Census of Employment and Wages (QCEW) provides detailed employment and wage data across various industries, states, and the entire U.S. For the first quarter of 2024, the dataset encompasses private sector employment and wages across all industries and establishment sizes. This comprehensive dataset, sourced from the Bureau of Labor Statistics, allows for an in-depth analysis of employment trends and wage patterns nationwide. The QCEW data is instrumental in understanding the economic landscape, providing valuable insights into employment distribution and wage structures across different sectors and regions.\nDashboard on Tableau",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#week-4",
    "href": "Pratiti.html#week-4",
    "title": "2  Pratiti",
    "section": "2.4 Week 4",
    "text": "2.4 Week 4\n\n2.4.1 Wednesday and Friday",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#this-is-a-markdown-title",
    "href": "Pratiti.html#this-is-a-markdown-title",
    "title": "2  Pratiti",
    "section": "2.5 This is a markdown title",
    "text": "2.5 This is a markdown title\nin markdown we can create lists:\n\nitem 1\nitem 2\nitem 3\n\nalso we can create enumerated list\n\nHola\nHi\nNamaste\n\nwe can do bold, also italic\n# Here we are importing numpy with a nickname np\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n1\n[1 2 3 4 5]\n# lists are native to python\nmy_list = [1, 2, 3, 4, 5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n# We will be using a lot of dataframes, so we need pandas library.\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#loading-csv-files",
    "href": "Pratiti.html#loading-csv-files",
    "title": "2  Pratiti",
    "section": "2.6 4. Loading csv files",
    "text": "2.6 4. Loading csv files\nTo load csv files into a DataFrame, we use the pandas function read_csv:\ndf = pd.read_csv('airquality_datasets.csv')\nNow, let’s load and explore the summary of the airquality dataset.\n# Summary of data frame\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#vizualizing-the-dataset",
    "href": "Pratiti.html#vizualizing-the-dataset",
    "title": "2  Pratiti",
    "section": "2.7 5. Vizualizing the dataset",
    "text": "2.7 5. Vizualizing the dataset\nLet’s dive into visualizations using matplotlib. We’ll start with simple histograms and boxplots, then move on to correlation plots.\n\n2.7.1 Histograms\nHistograms help us understand the distribution of the variables. We’ll create histograms for Ozone and Temp.\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#boxplots",
    "href": "Pratiti.html#boxplots",
    "title": "2  Pratiti",
    "section": "2.8 Boxplots",
    "text": "2.8 Boxplots\nBoxplots are useful for identifying outliers and understanding the spread of the data.\n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#correlation-plots",
    "href": "Pratiti.html#correlation-plots",
    "title": "2  Pratiti",
    "section": "2.9 Correlation Plots",
    "text": "2.9 Correlation Plots\nNext, we can explore the correlation between Ozone and Temp, using Month as a categorical variable.\nimport seaborn as sns\n\n# Scatter plot with regression line\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Temp', y='Ozone', hue='Month', data=df)\nplt.title('Temperature vs Ozone Levels by Month')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n# Correlation matrix\ncorr = df[['Ozone', 'Temp', 'Wind']].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Pratiti.html#week-5",
    "href": "Pratiti.html#week-5",
    "title": "2  Pratiti",
    "section": "2.10 Week 5",
    "text": "2.10 Week 5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pratiti</span>"
    ]
  },
  {
    "objectID": "Savannah.html",
    "href": "Savannah.html",
    "title": "3  Savannah",
    "section": "",
    "text": "3.1 Week 1\nexcel\n(note that I was not able to upload the photos to here) ### Wednesday\nACCESS THE WEB VERSION OF EXCEL THROUGH ONEDRIVE\n-cell is identified by a column letter and row number and holds a piece of data.\n-column is a vertical set of cells, identified by a letter.\n-row is a horizontal set of cells, identified by a number.\n=SUM() =AVERAGE() =MAX() =MIN()\ndouble click corner of box around a cell to make the formula go apply to the whole column\nuse control to select multiple specific cells/columns\nshift right control down to select\nuse &lt;&gt; to input a link in Quarto document\nuse to input a screenshot in a Quarto document MUST BE IN THE SAME FOLDER\nSCATTER PLOT EXAMPLE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Savannah</span>"
    ]
  },
  {
    "objectID": "Savannah.html#week-1",
    "href": "Savannah.html#week-1",
    "title": "3  Savannah",
    "section": "",
    "text": "BASIC FUNCTIONS\n\nEXCEL NOTES\n\n\n\n\n\nBASIC PLOTS\n\n\n3.1.1 Friday\nMy data set is Rates of COVID-19 Cases or Deaths by Age Group and Vaccination Status from the Centers for Disease Control and Prevention website.\nDESCRIPTION OF DATA SET\nThis data set was posted on October 21, 2022 and was revised on February 22, 2023. The data reflect cases among people with a positive specimen collection date through September 24, 2022, and deaths among people with a positive specimen collection date through September 3, 2022.\nThe data was provided by the CDC COVID-19 Response, Epidemiology Task Force.\nThis data set has 1,591 rows and 16 columns.\nCOLUMNS IN DATA SET\nOUTCOME: case or death (text)\nMONTH: corresponding month to the MMWR week- MMM YYYY (text)\nMMWR week: the morbidity and mortality weekly report- YYYYWW (text)\nAGE GROUP: 0-5 (4 yrs), 5-11, 12-17, 18-29, 30-49, 50-64, 65-70, 80+ (text)\nVACCINE PRODUCT: Janssen, Moderna, Pfizer, all (text)\nVACCINATED WITH OUTCOME: weekly count of vaccinated individuals that correspond with an outcome (number)\nFULLY VACCINATED POPULATION: cumulative weekly count of the population vaccinated (number)\nUNVACCINATED WITH OUTCOME: weekly count of unvaccinated individuals that correspond with an outcome (number)\nUNVACCINATED POPULATION: cumulative weekly count of the population un-vaccinated (number)\nCRUDE VAX IR: incident rate for vaccinated population per 100,000 (number)\nCRUDE UNVAX IR: incident rate for un-vaccinated population per 100,000 (number)\nCRUDE IRR: incident rate ratio (un-vaccinated : vaccinated) (number)\nAGE ADJUSTED VAX IR: incident rate by age for vaccinated population per 100,000 (number)\nAGE ADJUSTED UNVAX IR: incident rate by age for un-vaccinated population per 100,000 (number)\nAGE ADJUSTED CRUDE IRR: incident rate ratio by age (un-vaccinated : vaccinated) (number)\nCONTINUITY CORRECTION: indicates whether the data was adjusted with the assumption that at least 5 percent of the population in the jurisdiction is un-vaccinated (number)\ndataset link: https://data.cdc.gov/Public-Health-Surveillance/Rates-of-COVID-19-Cases-or-Deaths-by-Age-Group-and/3rge-nu2a/about_data\nVISUALIZATIONS IN EXCEL OF DATASET\nHISTOGRAM\nThis visualization shows the weekly count of vaccinated individuals who have either the outcome of a case of covid or death by covid, grouped by age.\nLINE PLOT\nThis visualization shows a cumulative weekly count of the fully vaccinated population (teal) versus the un-vaccinated population (orange).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Savannah</span>"
    ]
  },
  {
    "objectID": "Savannah.html#week-2",
    "href": "Savannah.html#week-2",
    "title": "3  Savannah",
    "section": "3.2 week 2",
    "text": "3.2 week 2\n\n3.2.0.1 EXCEL: PIVOT TABLE AND CHART\n\n\n3.2.0.2 Documentation\nI am using the airquality data set which lists the daily numeric values of Ozone, Solar Radiation, Wind, and Temperature from the months of May to September in 1973 from the state of New York. The Ozone data was measured from Roosevelt Island, the Solar Radiation data was measured from Central Park, and the both the Wind and Temperature data was measured from the LaGuardia airport. This data was obtained from the New York State Department of Conservation, which provided the Ozone data, as well as the National Weather Service, which provided the meteorological data including the temperature, wind, and solar radiation values.\nI used excel to clean the data by removing all the rows with NA values, and performed exploration analysis with histograms and a pivot table and chart.\n\n\n3.2.0.3 Ozone Histogram\nIn-class example\n\n\n\n3.2.0.4 Ozonve vs Temp Scatter Plot\nIn-class example\n\n\n\n3.2.0.5 My first Pivot Table and Chart\n\nFrom my pivot table, I was able to take all the numerical data from the airquality data set and use a stacked line chart to compare the values of Ozone, Solar Radiation, Wind, and Temperature per day in the month of May. The line chart is colored by topic to make it easy to visually compare each topic as it changes per day. From this visualization, I was able to observe that the Wind and Temperature values remained relatively even throughout the month, while the Solar Radiation values became occasionally unstable, and the Ozone values changed drastically throughout the month.\n\n\n3.2.0.6 INDIVIDUAL PROJECT DOCUMENTATION\nMy data set is Rates of COVID-19 Cases or Deaths by Age Group and Vaccination Status from the Centers for Disease Control and Prevention website.\n\n\n3.2.0.7 Description of data set\nThis data set was posted on October 21, 2022 and was revised on February 22, 2023. The data reflect cases among people within various age groups with a positive specimen collection date through September 24, 2022, and deaths among people within those age groups with a positive specimen collection date through September 3, 2022.\nThe data was provided by the CDC COVID-19 Response, Epidemiology Task Force.\nThis data set has 1,591 rows and 16 columns.\n\n\n3.2.0.8 Purpose of data set\nThis information can be used by the CDC and pharmaceutical companies to see how affective the vaccine is in preventing Covid-19 cases, as well as to see if the vaccine helps prevent death once the virus is contracted.\nThis information can also be used to see if the vaccine is more correlated to preventing deaths or cases within specific age groups.\nThis information can be used by hospitals to predict the necessary number of beds and ventilators needed for patients.\nThis information can be used by insurance companies to determine how much money should be put aside to cover health/life insurance costs.\n\n\n3.2.0.9 Variables in data set and data type\nOUTCOME: case or death (text)\nMONTH: corresponding month to the MMWR week- MMM YYYY (text)\nMMWR week: the morbidity and mortality weekly report- YYYYWW (text)\nAGE GROUP: 0-5 (4 yrs), 5-11, 12-17, 18-29, 30-49, 50-64, 65-70, 80+ (text)\nVACCINE PRODUCT: Janssen, Moderna, Pfizer, all (text)\nVACCINATED WITH OUTCOME: weekly count of vaccinated individuals that correspond with an outcome (number)\nFULLY VACCINATED POPULATION: cumulative weekly count of the population vaccinated (number)\nUNVACCINATED WITH OUTCOME: weekly count of unvaccinated individuals that correspond with an outcome (number)\nUNVACCINATED POPULATION: cumulative weekly count of the population un-vaccinated (number)\nCRUDE VAX IR: incident rate for vaccinated population per 100,000 (number)\nCRUDE UNVAX IR: incident rate for un-vaccinated population per 100,000 (number)\nCRUDE IRR: incident rate ratio (un-vaccinated : vaccinated) (number)\nAGE ADJUSTED VAX IR: incident rate by age for vaccinated population per 100,000 (number)\nAGE ADJUSTED UNVAX IR: incident rate by age for un-vaccinated population per 100,000 (number)\nAGE ADJUSTED CRUDE IRR: incident rate ratio by age (un-vaccinated : vaccinated) (number)\nCONTINUITY CORRECTION: indicates whether the data was adjusted with the assumption that at least 5 percent of the population in the jurisdiction is un-vaccinated (number)\n\n\n3.2.0.10 Link to data set\nhttps://data.cdc.gov/Public-Health-Surveillance/Rates-of-COVID-19-Cases-or-Deaths-by-Age-Group-and/3rge-nu2a/about_data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Savannah</span>"
    ]
  },
  {
    "objectID": "Savannah.html#week-3",
    "href": "Savannah.html#week-3",
    "title": "3  Savannah",
    "section": "3.3 Week 3",
    "text": "3.3 Week 3\n\n3.3.1 Wednesday\n\n\n3.3.2 Friday",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Savannah</span>"
    ]
  },
  {
    "objectID": "Savannah.html#week-4",
    "href": "Savannah.html#week-4",
    "title": "3  Savannah",
    "section": "3.4 Week 4",
    "text": "3.4 Week 4\n\n3.4.1 Wednesday and Friday\nimport numpy as np\narr = np.array([1,2,3,4,5])\nmy_list = [1, 2, 3, 4, 5]\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\ndf = pd.read_csv('C:\\\\Users\\\\rdudley\\\\Downloads\\\\airquality_datasets.csv')\n# Summary of the dataset\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng\n\n\n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng\n\n\nimport seaborn as sns\n\n# Scatter plot with regression line\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Temp', y='Ozone', hue='Month', data=df)\nplt.title('Temperature vs Ozone Levels by Month')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n# Correlation matrix\ncorr = df[['Ozone', 'Temp', 'Wind']].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\npng\n\n\n\n\n\npng",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Savannah</span>"
    ]
  },
  {
    "objectID": "Savannah.html#week-5",
    "href": "Savannah.html#week-5",
    "title": "3  Savannah",
    "section": "3.5 Week 5",
    "text": "3.5 Week 5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Savannah</span>"
    ]
  },
  {
    "objectID": "Savannah.html#week-after-fall-break",
    "href": "Savannah.html#week-after-fall-break",
    "title": "3  Savannah",
    "section": "3.6 Week after fall break",
    "text": "3.6 Week after fall break\n(For this weeks assignment, everyone needed to find a dataset from our datasource and give a summary about it.)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Savannah</span>"
    ]
  },
  {
    "objectID": "Shae.html",
    "href": "Shae.html",
    "title": "4  Shae",
    "section": "",
    "text": "4.1 week 1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shae</span>"
    ]
  },
  {
    "objectID": "Shae.html#week-1",
    "href": "Shae.html#week-1",
    "title": "4  Shae",
    "section": "",
    "text": "4.1.1 Monday\nTutorial on excel function, sum,average. Go here &lt;\nCell is…… column is… row is….\n\n4.1.1.1 Basic Functions\nsum =sum(cell:cell)\nScatter Plot Insert graph by the insert tab adding a screenshot into a quarto\n\n\n\n4.1.1.2 Our own document\nmtcars\n\n\n4.1.1.3 Using the x,Census Data\n\n\n4.1.1.4 Description\nIt is a census of the poverty levels in California in the year of 2018\n\n\n4.1.1.5 Format\nThis data has 63 rows and 4 columns. [1] Label -&gt; categorical [2]population estimate -&gt; numerical [3]below poverty population estimate -&gt; numerical [4]percent below poverty -&gt; percentage\n#3 details Label - categorizing of population by different variables i.e age, ethnicity, race and sex population estimate - calculation of total population by categories under label below poverty population estimate- calculation of total population below poverty levels below poverty by categories under label percentage - calculation of percent of total population divided by total population below poverty\nSource https://data.census.gov/table/ACSST1Y2018.S1701?q=poverty%20in%20california%20in%202018\n\n\n4.1.1.6 Cleaning\nI removed the ratios as it only had the total popualtion seperated by age which was just a reiteration of the first few rows\nthere was an option of including the marginal error and I chose against that. I did not believe it was vital in explaining my data.\n\n\n4.1.1.7 Explanation\nThis data is has 71 rows, which divides my data into groups that are in the category of age, sex, race,highest level of education,employment status, work experience,and UNRELATED INDIVIDUALS FOR WHOM POVERTY STATUS IS DETERMINED\nThis data is has 4 columns that are filled with label and three estimate, each estimate is separated into the total in the group identified in the row and the amount that is below poverty as well as the percentage of the amount below poverty compared to the total in the category highlighted in the rows.\n  The above are the visualizations I chose to do to explain my data. The first is the distribution of poverty levels by gender to get a better understanding of the poverty levels in each gender before we break it up even further\nThe second is by age and I chose to include the overall population of each age group to understand the poverty levels against the overall population\nI also then made a pivot table chart with the education levels which makes the chart interchangeable and I can single out each education levels poverty rate\nThe 4th is among ethnicity if I continue using this data set the poverty among ethnicity is very important to understand and I also used a pie chart because it was the easiest to see which ethnicity has the most and compare them against one another.\nTotal homeless population against the total population just helps understand the overall populations of both before the deep dive\nLastly i just did the poverty levels against age without the total population because I believe age is the best way to describe the poverty levels.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shae</span>"
    ]
  },
  {
    "objectID": "Shae.html#week-2",
    "href": "Shae.html#week-2",
    "title": "4  Shae",
    "section": "4.2 Week 2",
    "text": "4.2 Week 2\n\n4.2.0.1 Histograms\nI am using the air quality dataset, which is a measurement in New york from the months May to September 1973\n\n\n4.2.0.2 Format\nthis data has 153 observation and 6 variables but after cleaning it has 112 observations and six variables\n\n\n4.2.0.3 Source\nI got this data from d2l which was obtained from the New York State Department Of conservation\nI use excel to clean the data, remove all the NA which I used the filter tab to achieve.\n  This explains the average of ozone,solar,wind and temp pver the months which gives us a good idea of of which month was the highest and which had the stongest pull through the months.\n\nI chose maximum winds by the months and day to see their trend lind and which month going into specifically what day had the highest and lowest.\n I also did the average ozone by temp whichb gives an idea of what was happening in the ozone by different temps",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shae</span>"
    ]
  },
  {
    "objectID": "Shae.html#week-3",
    "href": "Shae.html#week-3",
    "title": "4  Shae",
    "section": "4.3 Week 3",
    "text": "4.3 Week 3\n\n4.3.1 Wednesday\nPuromycin was my data set which is the study of the reaction verus the substrate concentration in an enzymatic reaction involving untreated cells or cells treated with Puromycin.The experiment was conducted once with the enzyme treated with Puromycin, and once with the enzyme untreated.\nThis dataset has 23 rows and 3 columns\nconc -&gt; a numeric vector of substrate concentrations (parts per million-ppm)\nrate/velocity -&gt; a numeric vector of instantaneous reaction rates, which was calculated from conc (counts/min/min)\nstate -&gt; a factor with levels treated untreated\nhttps://public.tableau.com/authoring/Week_3_Tableu_Mukurazhizha/Dashboard1#1\n\n\n4.3.2 Friday\n-Shae plans to work on a dashboard based on poverty data for specifically the state of oklahoma between Friday and Saturday-\n\n4.3.2.1 Using the x,Census Data\n\n\n4.3.2.2 Description\nIt is a census of the poverty levels in Oklahoma in the year of 2023\n\n\n4.3.2.3 Format\nThis data has 6 fields and 69 rows [1] Label -&gt; categorical [2]population estimate -&gt; numerical [3]below poverty population estimate -&gt; numerical [4]percent below poverty -&gt; percentage\n#3 details Label - categorizing of population by different variables i.e age, ethnicity, race and sex population estimate - calculation of total population by categories under label below poverty population estimate- calculation of total population below poverty levels below poverty by categories under label percentage - calculation of percent of total population divided by total population below poverty\nSource https://data.census.gov/table/ACSST1Y2023.S1701?q=poverty%20in%20oklahoma%20in%202023\n\n\n4.3.2.4 Cleaning\nUpon putting my data into Tableu I had to change all my numerical data into numerical from categorical.\nDashboard Links https://public.tableau.com/authoring/Ok_2023/Dashboard1#2\n\n\n4.3.2.5 Explanation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shae</span>"
    ]
  },
  {
    "objectID": "Shae.html#week-4",
    "href": "Shae.html#week-4",
    "title": "4  Shae",
    "section": "4.4 Week 4",
    "text": "4.4 Week 4\n\n4.4.1 Wednesday and Friday\nimport sys\nprint(sys.prefix)\n/home/csc477_00/venv477\nimport numpy as np\narr = np.array([1, 2, 3, 4, 5])\nmy_list = [1, 2, 3, 4, 5]\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\ndf = pd.read_csv('path_to_file.csv')\n---------------------------------------------------------------------------\n\nFileNotFoundError                         Traceback (most recent call last)\n\nCell In[14], line 1\n----&gt; 1 df = pd.read_csv('path_to_file.csv')\n\n\nFile ~/venv477/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\n\nFile ~/venv477/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\n\nFile ~/venv477/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\n\nFile ~/venv477/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\n\nFile ~/venv477/lib/python3.11/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\n\nFileNotFoundError: [Errno 2] No such file or directory: 'path_to_file.csv'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shae</span>"
    ]
  },
  {
    "objectID": "Shae.html#week-5",
    "href": "Shae.html#week-5",
    "title": "4  Shae",
    "section": "4.5 Week 5",
    "text": "4.5 Week 5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shae</span>"
    ]
  },
  {
    "objectID": "Shae.html#week-after-fall-break",
    "href": "Shae.html#week-after-fall-break",
    "title": "4  Shae",
    "section": "4.6 Week after fall break",
    "text": "4.6 Week after fall break\n(For this weeks assignment, everyone needed to find a dataset from our datasource and give a summary about it.)\nThe dataset I chose is titled “Household Data Annual Averages” and it tracks the employment status of the civilian noninstitutional population in the U.S. from 1948 to the present. It includes various metrics in thousands of individuals, such as:\nCivilian Noninstitutional Population: Total number of people aged 16 and over not in military or institutional settings.\nCivilian Labor Force: Individuals who are either employed or unemployed, represented as a percentage of the population.\nEmployment and Unemployment: Counts and percentages of employed and unemployed individuals, differentiated by agriculture and nonagricultural industries.\nNot in Labor Force: Number of people not participating in the labor market.\nThe dataset highlights trends in labor force participation, employment rates, and unemployment over the decades, showing changes in economic conditions and labor market dynamics. For example, it reflects fluctuations in employment rates during economic recessions and expansions. The dataset is subject to revisions due to updates in population controls, which may impact historical comparisons.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Shae</span>"
    ]
  }
]